{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another workbook since the other one was slowing down.\n",
    "# load some prereq libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "import collections\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import word2vec\n",
    "from lda.lda import LDA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from pybrain.structure import FeedForwardNetwork\n",
    "from pybrain.structure import RecurrentNetwork\n",
    "from sklearn.svm import SVC\n",
    "from pybrain.structure import LinearLayer, SigmoidLayer\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from pybrain.structure import FullConnection\n",
    "from pybrain.datasets import ClassificationDataSet\n",
    "from pybrain.utilities           import percentError\n",
    "from pybrain.tools.shortcuts     import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules   import SoftmaxLayer\n",
    "import nltk\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################### Library Functions ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(e):\n",
    "    raw = e.decode('utf-8').lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text_no_stem(e):\n",
    "    raw = e.decode('utf-8').lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_num(ys):\n",
    "    y_true = []\n",
    "    for l in ys:\n",
    "        if l == 'Positive':\n",
    "            y_true.append(0)\n",
    "        if l == 'Neutral':\n",
    "            y_true.append(1)\n",
    "        if l == 'Negative':\n",
    "            y_true.append(2)\n",
    "    return y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_token_vector(tokens):\n",
    "    v = np.zeros(len(vocabulary_set))\n",
    "    for t in tokens:\n",
    "        v[vocabulary_set.index(t)] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_vector(tokens, candidate, location, subject):\n",
    "    # returns the feature vector that represents this\n",
    "    v = np.zeros(len(vocabulary_set) + 3)\n",
    "    for t in tokens:\n",
    "        v[vocabulary_set.index(t)] = 1\n",
    "    # get candidate id\n",
    "    v[len(vocabulary_set)] = candidate_set.index(candidate)\n",
    "    v[len(vocabulary_set) + 1] = location_set.index(location)\n",
    "    v[len(vocabulary_set) + 2] = subject_set.index(subject)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_vector_expanded(tokens, candidate, location, subject):\n",
    "    # returns the feature vector that represents this\n",
    "    v = np.zeros(len(vocabulary_set) + len(candidate_set) + len(subject_set) + len(location_set))\n",
    "    for t in tokens:\n",
    "        v[vocabulary_set.index(t)] = 1\n",
    "\n",
    "    # get candidate id\n",
    "    v[len(vocabulary_set) + candidate_set.index(candidate)] = 1\n",
    "    v[len(vocabulary_set) + len(candidate_set) + location_set.index(location)] = 1\n",
    "    v[len(vocabulary_set) + len(candidate_set) + len(location_set) + subject_set.index(subject)] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_vector_cls(candidate, location, subject):\n",
    "    v = np.zeros(len(candidate_set) + len(subject_set) + len(location_set))\n",
    "    \n",
    "    # get candidate id\n",
    "    v[candidate_set.index(candidate)] = 1\n",
    "    v[len(candidate_set) + location_set.index(location)] = 1\n",
    "    v[len(candidate_set) + len(location_set) + subject_set.index(subject)] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_vector_pos(tokens):\n",
    "    v = np.zeros(len(pos_set))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    for w, t in tags:\n",
    "        v[pos_sl.index(t)] = v[pos_sl.index(t)] + 1\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_location_vector(l):\n",
    "    v = np.zeros(len(location_set))\n",
    "    v[location_set.index(l)] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subject_vector(sm):\n",
    "    v = np.zeros(len(subject_set))\n",
    "    v[subject_set.index(sm)] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [\"Positive\", \"Neutral\", \"Negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################## End Library Functions #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['PRP$', 'VBG', 'VBD', 'VBN', 'VBP', 'WDT', 'JJ', 'WP', 'VBZ', 'DT', 'RP', '$', 'NN', 'FW', 'TO', 'PRP', 'RB', 'NNS', 'NNP', 'VB', 'WRB', 'CC', 'PDT', 'RBS', 'RBR', 'CD', 'EX', 'IN', 'WP$', 'MD', 'NNPS', 'JJS', 'JJR', 'SYM', 'UH'])\n"
     ]
    }
   ],
   "source": [
    "pos_set = set()\n",
    "for s in dftrain['text']:\n",
    "#     print s\n",
    "    raw = s.decode('utf-8').lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "#     print tags\n",
    "    for w, t in tags:\n",
    "        pos_set.add(t)\n",
    "print pos_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_sl = list(pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('output/Sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dftrain['text']\n",
    "texts_all = []\n",
    "for e in df:\n",
    "    raw = e.decode('utf-8').lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts_all.append(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = [item for sublist in texts_all for item in sublist]\n",
    "# print vocabulary\n",
    "vocabulary_set = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidate_list = []\n",
    "for c in dftrain['candidate']:\n",
    "    candidate_list.append(c)\n",
    "candidate_set = list(set(candidate_list))\n",
    "\n",
    "subject_list = []\n",
    "for s in dftrain['subject_matter']:\n",
    "    subject_list.append(s)\n",
    "subject_set = list(set(subject_list))\n",
    "\n",
    "location_list = []\n",
    "for l in dftrain['tweet_location']:\n",
    "    location_list.append(l)\n",
    "location_set = list(set(location_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a partial set\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(dftrain['text'], dftrain['sentiment'],\n",
    "                                                                    test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build an LDA model using this X_train set\n",
    "m = np.array([np.zeros(len(vocabulary_set)) for i in range(len(X_train))])\n",
    "for i in range(len(X_train)):\n",
    "    tokens = process_text(X_train.iloc[i])\n",
    "    for t in tokens:\n",
    "        m[i][vocabulary_set.index(t)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x10b700b00>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have a matrix, lets use LDA on it.\n",
    "model = LDA(n_topics=10, random_state=0)\n",
    "model.fit(m.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8322, 10)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doc_topic_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have the LDA vectors, which represent the probability of each topic. Lets feed this in as a matrix to a classifier\n",
    "# see how the classifier performs given the dimension reduction that has happened.\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(model.doc_topic_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(dftrain['sentiment'],n_folds=10)\n",
    "averageError = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434125269978\n",
      "Average error: 0.00%\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "avg_cm = np.zeros((3, 3))\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    m = np.array([np.zeros(len(vocabulary_set)) for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text(X_train.iloc[i])\n",
    "        for t in tokens:\n",
    "            m[i][vocabulary_set.index(t)] = 1\n",
    "    \n",
    "    model = LDA(n_topics=13, random_state=0)\n",
    "    model.fit(m.astype(int))\n",
    "    \n",
    "    m_test = np.array([np.zeros(len(vocabulary_set)) for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text(X_test.iloc[i])\n",
    "        for t in tokens:\n",
    "            m_test[i][vocabulary_set.index(t)] = 1\n",
    "            \n",
    "    clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(model.doc_topic_, y_train)\n",
    "    y_pred = clf.predict(clf_X_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    avg_cm += confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(avg_cm / 1.0, title=\"Confusion Matrix - RF + LDA\")\n",
    "    plt.show()\n",
    "    break\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')\n",
    "plot_confusion_matrix(avg_cm / k, title=\"Confusion Matrix - RF + LDA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13871\n"
     ]
    }
   ],
   "source": [
    "print len(dftrain['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13871\n"
     ]
    }
   ],
   "source": [
    "print len(dftrain['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "for train_index, test_index in skf:\n",
    "    print dftrain['subject_matter'][train_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.392368610511\n",
      "0.389488840893\n",
      "0.39409221902\n",
      "0.419610670512\n",
      "0.416005767844\n",
      "0.426820475847\n",
      "0.352092352092\n",
      "0.391053391053\n",
      "0.348484848485\n",
      "0.367965367965\n",
      "Average error: 38.98%\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "avg_cm = np.zeros((3,3))\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    z_array = np.zeros(len(pos_set) + len(location_set) + len(subject_set))\n",
    "    m = np.array([z_array for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text_no_stem(X_train.iloc[i])\n",
    "#         candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        v = np.concatenate((create_feature_vector_pos(tokens), create_location_vector(tweet_location)))\n",
    "        m[i] = np.concatenate((v, create_subject_vector(sm)))\n",
    "    \n",
    "#     model = LDA(n_topics=5, random_state=0)\n",
    "#     model.fit(m.astype(int))\n",
    "    \n",
    "    z_array = np.zeros(len(pos_set) + len(location_set) + len(subject_set))\n",
    "    m_test = np.array([z_array for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text_no_stem(X_test.iloc[i])\n",
    "#         candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        v = np.concatenate((create_feature_vector_pos(tokens), create_location_vector(tweet_location)))\n",
    "        m_test[i] = np.concatenate((v, create_subject_vector(sm)))\n",
    "            \n",
    "#     clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(m, y_train)\n",
    "    y_pred = clf.predict(m_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    averageError += (1./k) * error\n",
    "    avg_cm += confusion_matrix(y_test, y_pred)\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')\n",
    "plot_confusion_matrix(avg_cm / k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.     0.     0. ...,     8.  1962.    12.]\n",
      " [    0.     0.     0. ...,    11.  2078.    12.]\n",
      " [    0.     0.     0. ...,     8.  3307.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     8.  1670.    12.]\n",
      " [    0.     0.     0. ...,     5.  2691.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    11.]]\n",
      "0.347732181425\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    1.96200000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   1.10000000e+01\n",
      "    2.07800000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    3.30700000e+03   1.20000000e+01]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   5.00000000e+00\n",
      "    1.38900000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   5.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   1.00000000e+01\n",
      "    1.21700000e+03   3.00000000e+00]]\n",
      "0.347732181425\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   3.00000000e+00\n",
      "    1.61900000e+03   2.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   3.00000000e+00\n",
      "    2.98300000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    8.13000000e+02   9.00000000e+00]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   5.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    2.07800000e+03   1.00000000e+00]]\n",
      "0.353025936599\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    4.09100000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   6.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    3.52800000e+03   3.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   1.00000000e+00\n",
      "    8.40000000e+02   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   6.00000000e+00\n",
      "    4.20100000e+03   2.00000000e+00]]\n",
      "0.366258111031\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     8.  1355.    12.]\n",
      " [    0.     0.     0. ...,     5.  1619.    12.]\n",
      " [    0.     0.     0. ...,     8.  4243.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     8.     0.     5.]\n",
      " [    0.     0.     0. ...,     8.  1222.     5.]\n",
      " [    0.     0.     0. ...,     7.  3777.    12.]]\n",
      "0.360490266763\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     6.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.  1962.    12.]\n",
      " [    0.     0.     0. ...,    11.     0.     9.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     8.  3748.     5.]\n",
      " [    0.     0.     0. ...,     8.  3208.     5.]\n",
      " [    0.     0.     0. ...,     8.     0.     5.]]\n",
      "0.421773612112\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    1.61900000e+03   1.00000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    1.61900000e+03   1.20000000e+01]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    0.00000000e+00   5.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    3.68600000e+03   5.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   3.00000000e+00\n",
      "    0.00000000e+00   5.00000000e+00]]\n",
      "0.354978354978\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     8.   139.    12.]\n",
      " [    0.     0.     0. ...,     8.  1947.    12.]\n",
      " [    0.     0.     0. ...,     8.  3408.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     8.    97.     5.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.     5.]]\n",
      "0.381673881674\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     8.  1909.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.     3.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     5.  2557.    12.]\n",
      " [    0.     0.     0. ...,     3.   360.    12.]\n",
      " [    0.     0.     0. ...,     8.  2972.    12.]]\n",
      "0.324675324675\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     5.  2557.    12.]\n",
      " [    0.     0.     0. ...,     3.   360.    12.]\n",
      " [    0.     0.     0. ...,     8.  2972.    12.]]\n",
      "[[    0.     0.     0. ...,     6.     0.    11.]\n",
      " [    0.     0.     0. ...,     8.   166.     6.]\n",
      " [    0.     0.     0. ...,     3.     0.     5.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "0.351370851371\n",
      "Average error: 36.10%\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    m = np.array([np.zeros(len(vocabulary_set) + 3) for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        m[i] = create_feature_vector(tokens, candidate, tweet_location, sm)\n",
    "    print m\n",
    "#     model = LDA(n_topics=13, random_state=0)\n",
    "#     model.fit(m.astype(int))\n",
    "    \n",
    "    m_test = np.array([np.zeros(len(vocabulary_set) + 3) for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        m_test[i] = create_feature_vector(tokens, candidate, tweet_location, sm)\n",
    "    print m_test\n",
    "#     clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(m, y_train)\n",
    "    y_pred = clf.predict(m_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    m = np.array([np.zeros(len(vocabulary_set) + 3) for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        m[i] = create_feature_vector(tokens, candidate, tweet_location, sm)\n",
    "    print m\n",
    "#     model = LDA(n_topics=13, random_state=0)\n",
    "#     model.fit(m.astype(int))\n",
    "    \n",
    "    m_test = np.array([np.zeros(len(vocabulary_set) + 3) for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        m_test[i] = create_feature_vector(tokens, candidate, tweet_location, sm)\n",
    "    print m_test\n",
    "#     clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = SVC(kernel=chi2_kernel)\n",
    "    clf.fit(m, y_train)\n",
    "    y_pred = clf.predict(m_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## building a neural network system to test with here as well.\n",
    "def createNetwork(inputDim, hiddenDim, outputDim):\n",
    "    n = FeedForwardNetwork()\n",
    "    n.addInputModule(LinearLayer(inputDim, name='in'))\n",
    "    n.addModule(SigmoidLayer(hiddenDim, name='hidden'))\n",
    "    n.addModule(SigmoidLayer(hiddenDim, name='hidden1'))\n",
    "    n.addModule(SigmoidLayer(hiddenDim, name='hidden2'))\n",
    "    n.addOutputModule(SoftmaxLayer(outputDim, name='out'))\n",
    "    n.addConnection(FullConnection(n['in'], n['hidden'], name='c1'))\n",
    "    n.addConnection(FullConnection(n['hidden'], n['hidden1'], name='c2'))\n",
    "    n.addConnection(FullConnection(n['hidden1'], n['hidden2'], name='c3'))\n",
    "    n.addConnection(FullConnection(n['hidden2'], n['out'], name='c4'))\n",
    "    n.sortModules()\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: dim(16382, 300)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  2.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  2.  0.]\n",
      " [ 0.  0.  0. ...,  1.  1.  0.]]\n",
      "\n",
      "target: dim(16382, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [2]\n",
      " [0]]\n",
      "\n",
      "class: dim(0, 1)\n",
      "[]\n",
      "\n",
      "\n",
      "input: dim(254, 300)\n",
      "[[ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  1.  2.  1.]\n",
      " [ 0.  0.  0. ...,  0.  2.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  4.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "\n",
      "target: dim(254, 1)\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "\n",
      "class: dim(0, 1)\n",
      "[]\n",
      "\n",
      "\n",
      "FeedForwardNetwork-41\n",
      "   Modules:\n",
      "    [<LinearLayer 'in'>, <SigmoidLayer 'hidden'>, <SigmoidLayer 'hidden1'>, <SigmoidLayer 'hidden2'>, <SoftmaxLayer 'out'>]\n",
      "   Connections:\n",
      "    [<FullConnection 'c1': 'in' -> 'hidden'>, <FullConnection 'c2': 'hidden' -> 'hidden1'>, <FullConnection 'c3': 'hidden1' -> 'hidden2'>, <FullConnection 'c4': 'hidden2' -> 'out'>]\n",
      "\n",
      "Training Epoch: 0\n",
      "Total error: 0.10203577461\n",
      "epoch:    1   train error: 36.21%   test error: 40.71%\n",
      "Training Epoch: 1\n",
      "Total error: 0.0934353848474\n",
      "epoch:    2   train error: 35.45%   test error: 37.86%\n",
      "Training Epoch: 2\n",
      "Total error: 0.090058370646\n",
      "epoch:    3   train error: 34.75%   test error: 45.00%\n",
      "Training Epoch: 3\n",
      "Total error: 0.0886350563899\n",
      "epoch:    4   train error: 42.41%   test error: 55.71%\n",
      "Training Epoch: 4\n",
      "Total error: 0.0870551396267\n",
      "epoch:    5   train error: 37.10%   test error: 39.29%\n",
      "Training Epoch: 5\n",
      "Total error: 0.0861507446543\n",
      "epoch:    6   train error: 31.48%   test error: 42.86%\n",
      "Training Epoch: 6\n",
      "Total error: 0.0846169064679\n",
      "epoch:    7   train error: 34.29%   test error: 46.43%\n",
      "Training Epoch: 7\n",
      "Total error: 0.083043857634\n",
      "epoch:    8   train error: 30.41%   test error: 40.00%\n",
      "Training Epoch: 8\n",
      "Total error: 0.0823852447723\n",
      "epoch:    9   train error: 35.08%   test error: 46.43%\n",
      "Training Epoch: 9\n",
      "Total error: 0.0808104001712\n",
      "epoch:   10   train error: 31.45%   test error: 37.86%\n",
      "Training Epoch: 10\n",
      "Total error: 0.0802343134328\n",
      "epoch:   11   train error: 30.76%   test error: 36.43%\n",
      "Training Epoch: 11\n",
      "Total error: 0.0787165380169\n",
      "epoch:   12   train error: 34.08%   test error: 37.86%\n",
      "Training Epoch: 12\n",
      "Total error: 0.0782126710557\n",
      "epoch:   13   train error: 32.75%   test error: 41.43%\n",
      "Training Epoch: 13\n",
      "Total error: 0.078064196732\n",
      "epoch:   14   train error: 32.52%   test error: 36.43%\n",
      "Training Epoch: 14\n",
      "Total error: 0.0765219766585\n",
      "epoch:   15   train error: 30.07%   test error: 36.43%\n",
      "Training Epoch: 15\n",
      "Total error: 0.0762684359408\n",
      "epoch:   16   train error: 37.46%   test error: 39.29%\n",
      "Training Epoch: 16\n",
      "Total error: 0.0753364194878\n",
      "epoch:   17   train error: 32.84%   test error: 35.71%\n",
      "Training Epoch: 17\n",
      "Total error: 0.0746888745894\n",
      "epoch:   18   train error: 27.75%   test error: 35.00%\n",
      "Training Epoch: 18\n",
      "Total error: 0.0735628275275\n",
      "epoch:   19   train error: 34.14%   test error: 36.43%\n",
      "Training Epoch: 19\n",
      "Total error: 0.0732929787709\n",
      "epoch:   20   train error: 26.94%   test error: 36.43%\n",
      "Training Epoch: 20\n",
      "Total error: 0.0715319997975\n",
      "epoch:   21   train error: 28.20%   test error: 32.86%\n",
      "Training Epoch: 21\n",
      "Total error: 0.0713559557124\n",
      "epoch:   22   train error: 32.34%   test error: 35.00%\n",
      "Training Epoch: 22\n",
      "Total error: 0.0714675887\n",
      "epoch:   23   train error: 34.43%   test error: 35.00%\n",
      "Training Epoch: 23\n",
      "Total error: 0.0700019081157\n",
      "epoch:   24   train error: 30.02%   test error: 35.71%\n",
      "Training Epoch: 24\n",
      "Total error: 0.0700824017748\n",
      "epoch:   25   train error: 29.73%   test error: 45.00%\n",
      "Training Epoch: 25\n",
      "Total error: 0.0699174068312\n",
      "epoch:   26   train error: 45.06%   test error: 60.71%\n",
      "Training Epoch: 26\n",
      "Total error: 0.0688065210211\n",
      "epoch:   27   train error: 24.73%   test error: 37.14%\n",
      "Training Epoch: 27\n",
      "Total error: 0.0682470970875\n",
      "epoch:   28   train error: 32.93%   test error: 35.00%\n",
      "Training Epoch: 28\n",
      "Total error: 0.0688125048925\n",
      "epoch:   29   train error: 38.72%   test error: 56.43%\n",
      "Training Epoch: 29\n",
      "Total error: 0.0677237299929\n",
      "epoch:   30   train error: 32.21%   test error: 52.86%\n",
      "Training Epoch: 30\n",
      "Total error: 0.0672791247848\n",
      "epoch:   31   train error: 28.69%   test error: 31.43%\n",
      "Training Epoch: 31\n",
      "Total error: 0.066084176077\n",
      "epoch:   32   train error: 25.17%   test error: 37.14%\n",
      "Training Epoch: 32\n",
      "Total error: 0.0654757172298\n",
      "epoch:   33   train error: 49.38%   test error: 61.43%\n",
      "Training Epoch: 33\n",
      "Total error: 0.0661239925963\n",
      "epoch:   34   train error: 27.44%   test error: 42.86%\n",
      "Training Epoch: 34\n",
      "Total error: 0.0652139439959\n",
      "epoch:   35   train error: 34.13%   test error: 38.57%\n",
      "Training Epoch: 35\n",
      "Total error: 0.0645349272481\n",
      "epoch:   36   train error: 22.63%   test error: 37.14%\n",
      "Training Epoch: 36\n",
      "Total error: 0.0644233110259\n",
      "epoch:   37   train error: 26.18%   test error: 40.00%\n",
      "Training Epoch: 37\n",
      "Total error: 0.0638864076997\n",
      "epoch:   38   train error: 40.33%   test error: 61.43%\n",
      "Training Epoch: 38\n",
      "Total error: 0.0635696300705\n",
      "epoch:   39   train error: 25.20%   test error: 38.57%\n",
      "Training Epoch: 39\n",
      "Total error: 0.0629614225006\n",
      "epoch:   40   train error: 29.82%   test error: 35.71%\n",
      "Training Epoch: 40\n",
      "Total error: 0.0627510047353\n",
      "epoch:   41   train error: 22.81%   test error: 35.00%\n",
      "Training Epoch: 41\n",
      "Total error: 0.0626611410484\n",
      "epoch:   42   train error: 26.84%   test error: 35.00%\n",
      "Training Epoch: 42\n",
      "Total error: 0.0623128783031\n",
      "epoch:   43   train error: 22.53%   test error: 33.57%\n",
      "Training Epoch: 43\n",
      "Total error: 0.0621225718643\n",
      "epoch:   44   train error: 30.04%   test error: 37.86%\n",
      "Training Epoch: 44\n",
      "Total error: 0.0617945545088\n",
      "epoch:   45   train error: 24.75%   test error: 36.43%\n",
      "Training Epoch: 45\n",
      "Total error: 0.061961648982\n",
      "epoch:   46   train error: 25.80%   test error: 33.57%\n",
      "Training Epoch: 46\n",
      "Total error: 0.0613106514144\n",
      "epoch:   47   train error: 38.35%   test error: 60.00%\n",
      "Training Epoch: 47\n",
      "Total error: 0.0612029266121\n",
      "epoch:   48   train error: 44.02%   test error: 56.43%\n",
      "Training Epoch: 48\n",
      "Total error: 0.0605005295919\n",
      "epoch:   49   train error: 24.08%   test error: 32.86%\n",
      "Training Epoch: 49\n",
      "Total error: 0.0607694562746\n",
      "epoch:   50   train error: 25.88%   test error: 33.57%\n",
      "Average error: 33.57%\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    input_dim = 300\n",
    "    trndata = ClassificationDataSet(input_dim, nb_classes=3)\n",
    "    z_array = np.zeros(len(vocabulary_set) + len(candidate_set) + len(location_set) + len(subject_set) + len(pos_set))\n",
    "#     z_array = np.zeros(len(pos_set))\n",
    "    m = np.array([z_array for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text_no_stem(X_train.iloc[i])\n",
    "        tokens_stemmed = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        l = label_to_num([y_train.iloc[i]])[0]\n",
    "        v1 = create_feature_vector_expanded(tokens_stemmed, candidate, tweet_location, sm)\n",
    "        v2 = create_feature_vector_pos(tokens)\n",
    "#         trndata.addSample(np.concatenate((v1,v2)), [l])\n",
    "        m[i] = np.concatenate((v1, v2))\n",
    "#         m[i] = v2\n",
    "    \n",
    "    # feature select on this m\n",
    "    selectkb = SelectKBest(f_classif, k=input_dim)\n",
    "    m_new = selectkb.fit_transform(m, y_train)\n",
    "    \n",
    "    for i in range(len(m_new)):\n",
    "        l = label_to_num([y_train.iloc[i]])[0]\n",
    "        trndata.addSample(m_new[i], [l])\n",
    "    \n",
    "    print trndata    \n",
    "    \n",
    "    testdata = ClassificationDataSet(input_dim, nb_classes=3)\n",
    "    z_array = np.zeros(len(vocabulary_set) + len(candidate_set) + len(location_set) + len(subject_set) + len(pos_set))\n",
    "#     z_array = np.zeros(len(pos_set))\n",
    "    m_test = np.array([z_array for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text_no_stem(X_test.iloc[i])\n",
    "        tokens_stemmed = process_text(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        l = label_to_num([y_test.iloc[i]])[0]\n",
    "        v1 = create_feature_vector_expanded(tokens_stemmed, candidate, tweet_location, sm)\n",
    "        v2 = create_feature_vector_pos(tokens)\n",
    "#         testdata.addSample(np.concatenate((v1,v2)), [l])\n",
    "        m_test[i] = np.concatenate((v1, v2))\n",
    "#         m_test[i] = v2\n",
    "    \n",
    "    # now transform and add to testdata\n",
    "    m_test_new = selectkb.transform(m_test)\n",
    "    for i in range(len(m_test)):\n",
    "        l = label_to_num([y_test.iloc[i]])[0]\n",
    "        testdata.addSample(m_test_new[i], [l])\n",
    "    \n",
    "    print testdata\n",
    "    # convert to one of many\n",
    "    trndata._convertToOneOfMany()\n",
    "    testdata._convertToOneOfMany()\n",
    "    # train the network\n",
    "    n = createNetwork(trndata.indim, 200, trndata.outdim)\n",
    "    print n\n",
    "    trainer = BackpropTrainer(n, dataset=trndata, momentum=0.1, verbose=True, weightdecay=0.0001)\n",
    "    \n",
    "    # train for a certain amount of epochs\n",
    "    epoch_list = []\n",
    "    test_error_list = []\n",
    "    train_error_list = []\n",
    "    for i in range(50):\n",
    "        print \"Training Epoch: {0}\".format(i)\n",
    "        trainer.trainEpochs(1)\n",
    "        trnresult = percentError( trainer.testOnClassData(),\n",
    "                              trndata['class'] )\n",
    "        tstresult = percentError( trainer.testOnClassData(\n",
    "               dataset=testdata ), testdata['class'] )\n",
    "        epoch_list.append(i)\n",
    "        test_error_list.append(tstresult)\n",
    "        train_error_list.append(trnresult)\n",
    "        print \"epoch: %4d\" % trainer.totalepochs, \\\n",
    "              \"  train error: %5.2f%%\" % trnresult, \\\n",
    "              \"  test error: %5.2f%%\" % tstresult\n",
    "    error = percentError(trainer.testOnClassData(dataset=testdata), testdata['class'])\n",
    "    averageError += (1./k) * error\n",
    "    out = n.activateOnDataset(testdata)\n",
    "    out = out.argmax(axis=1)\n",
    "    cm = confusion_matrix(label_to_num(y_test), out)\n",
    "    plot_confusion_matrix(cm, title=\"Confusion Matrix: NN with F-Test\")\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_list, test_error_list, color=\"red\")\n",
    "    plt.plot(epoch_list, train_error_list, color=\"green\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(\"Error as a Function of Epochs Trained\")\n",
    "    plt.show()\n",
    "    break\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  1.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  2.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [   33    35    36 ..., 19670 19678 19679] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.372108108108\n",
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  2.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [   38    47    49 ..., 19682 19685 19689] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  1.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "0.425048669695\n",
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [   40    55    75 ..., 19683 19687 19688] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  1.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  2.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "0.343067272334\n",
      "Average error: 38.01%\n"
     ]
    }
   ],
   "source": [
    "# POS_TAG RF classifier\n",
    "k = 3\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "avg_cm = np.zeros((3,3))\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    z_array = np.zeros(len(vocabulary_set) + len(candidate_set) + len(location_set) + len(subject_set) + len(pos_set))\n",
    "    m = np.array([z_array for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text_no_stem(X_train.iloc[i])\n",
    "        tokens_stemmed = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        v1 = create_feature_vector_expanded(tokens_stemmed, candidate, tweet_location, sm)\n",
    "        m[i] = np.concatenate((create_feature_vector_pos(tokens), v1))\n",
    "    print m\n",
    "#     model = LDA(n_topics=13, random_state=0)\n",
    "#     model.fit(m.astype(int))\n",
    "\n",
    "    selectkb = SelectKBest(f_classif, k=150)\n",
    "    m_new = selectkb.fit_transform(m, y_train)\n",
    "    \n",
    "    z_array = np.zeros(len(vocabulary_set) + len(candidate_set) + len(location_set) + len(subject_set) + len(pos_set))\n",
    "    m_test = np.array([z_array for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text_no_stem(X_test.iloc[i])\n",
    "        tokens_stemmed = process_text(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        v1 = create_feature_vector_expanded(tokens_stemmed, candidate, tweet_location, sm)\n",
    "        m_test[i] = np.concatenate((create_feature_vector_pos(tokens), v1))\n",
    "    print m_test\n",
    "#     clf_X_test = model.transform(m_test.astype(int))\n",
    "\n",
    "    m_test_new = selectkb.transform(m_test)\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(m, y_train)\n",
    "    y_pred = clf.predict(m_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    avg_cm += confusion_matrix(y_test, y_pred)\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')\n",
    "plot_confusion_matrix(avg_cm / k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.544276457883\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-90ed1cd602a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdftrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_matter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feature_vector_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feature_vector_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-b04253df5d46>\u001b[0m in \u001b[0;36mcreate_feature_vector_pos\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_feature_vector_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_sl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_sl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[1;32m    110\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-ptb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    z_array = np.zeros(len(candidate_set) + len(location_set) + len(subject_set) + len(pos_set))\n",
    "    m = np.array([z_array for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text_no_stem(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        v1 = create_feature_vector_cls(candidate, tweet_location, sm)\n",
    "        v2 = create_feature_vector_pos(tokens)\n",
    "        m[i] = np.concatenate((v1, v2))\n",
    "    \n",
    "    model = LDA(n_topics=15, random_state=0)\n",
    "    model.fit(m.astype(int))\n",
    "    \n",
    "    z_array = np.zeros(len(candidate_set) + len(location_set) + len(subject_set) + len(pos_set))\n",
    "    m_test = np.array([z_array for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text_no_stem(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        v1 = create_feature_vector_cls(candidate, tweet_location, sm)\n",
    "        v2 = create_feature_vector_pos(tokens)\n",
    "        m_test[i] = np.concatenate((v1, v2))\n",
    "            \n",
    "    clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(model.doc_topic_, y_train)\n",
    "    y_pred = clf.predict(clf_X_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  2.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "[  15.34051944   60.12570275   41.50017614 ...,   11.39547514  106.37947709\n",
      "  281.65913004]\n",
      "DescribeResult(nobs=array(3997), minmax=(masked_array(data = 0.047994131649,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", masked_array(data = 359.513441906,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      "), mean=3.2387702681422308, variance=209.28695978159089, skewness=masked_array(data = 16.6895913072,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", kurtosis=316.27154602208469)\n",
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  2.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "[  16.60920519   64.41464555   42.59031343 ...,   10.31995046  113.46744237\n",
      "  265.10517266]\n",
      "DescribeResult(nobs=array(4030), minmax=(masked_array(data = 0.00560422083874,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", masked_array(data = 385.817034858,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      "), mean=3.2164263710415244, variance=210.36050797017566, skewness=masked_array(data = 16.9768870311,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", kurtosis=331.28721560239779)\n",
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  2.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "[  18.54890331   73.87018276   40.75238528 ...,    8.85111654  107.57958218\n",
      "  295.48470052]\n",
      "DescribeResult(nobs=array(4042), minmax=(masked_array(data = 0.00556520139256,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", masked_array(data = 388.80946263,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      "), mean=3.2021870141481479, variance=208.13216568396425, skewness=masked_array(data = 17.2719990068,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", kurtosis=344.58446026588996)\n",
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  2.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "[  22.00482986   73.0274938    50.69202856 ...,   12.26083113  109.19079842\n",
      "  290.02464287]\n",
      "DescribeResult(nobs=array(4010), minmax=(masked_array(data = 0.0465536237737,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", masked_array(data = 417.044149828,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      "), mean=3.2665096051594484, variance=225.8385708878296, skewness=masked_array(data = 17.2817244943,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", kurtosis=346.97531941920846)\n",
      "[[ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  2.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "[  22.03089907   79.68079121   29.17796847 ...,   12.35087677  103.42777758\n",
      "  285.01242205]\n",
      "DescribeResult(nobs=array(4012), minmax=(masked_array(data = 0.0478809104776,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", masked_array(data = 396.869543861,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      "), mean=3.2323318971843666, variance=212.59343305436252, skewness=masked_array(data = 16.9381120062,\n",
      "             mask = False,\n",
      "       fill_value = 1e+20)\n",
      ", kurtosis=332.15861148554615)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-14f49e11ecd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdftrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_matter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feature_vector_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_feature_vector_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     model = LDA(n_topics=13, random_state=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-b04253df5d46>\u001b[0m in \u001b[0;36mcreate_feature_vector_pos\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_feature_vector_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_sl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_sl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[1;32m    110\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-ptb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mprev2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abkhanna/Documents/workspace/Princeton/cos424/a2/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Feature selection time\n",
    "# Feature selection time\n",
    "# POS_TAG RF classifier\n",
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    z_array = np.zeros(len(candidate_set) + len(location_set) + len(subject_set) + len(pos_set))\n",
    "    m = np.array([z_array for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text_no_stem(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        v1 = create_feature_vector_cls(candidate, tweet_location, sm)\n",
    "        m[i] = np.concatenate((create_feature_vector_pos(tokens), v1))\n",
    "    print m\n",
    "#     model = LDA(n_topics=13, random_state=0)\n",
    "#     model.fit(m.astype(int))\n",
    "\n",
    "    skbest = SelectKBest(f_classif, k=100)\n",
    "    skbest.fit(m, y_train)\n",
    "    print skbest.scores_\n",
    "    print describe(skbest.scores_, nan_policy='omit')\n",
    "    \n",
    "#     clf = RandomForestClassifier(random_state=0)\n",
    "#     clf.fit(m, y_train)\n",
    "#     y_pred = clf.predict(m_test)\n",
    "#     error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "#     print error\n",
    "#     averageError += (1./k) * error\n",
    "# print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-404c1a0d833d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtweet_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdftrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdftrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_matter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feature_vector_expanded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_stemmed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_feature_vector_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-de7fb6abe05e>\u001b[0m in \u001b[0;36mcreate_feature_vector_expanded\u001b[0;34m(tokens, candidate, location, subject)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# get candidate id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Feature selection time\n",
    "# Feature selection time\n",
    "# POS_TAG RF classifier\n",
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    z_array = np.zeros(len(vocabulary_set) + len(candidate_set) + len(location_set) + len(subject_set) + len(pos_set))\n",
    "    m = np.array([z_array for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text_no_stem(X_train.iloc[i])\n",
    "        tokens_stemmed = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        v1 = create_feature_vector_expanded(tokens_stemmed, candidate, tweet_location, sm)\n",
    "        m[i] = np.concatenate((create_feature_vector_pos(tokens), v1))\n",
    "    print m\n",
    "#     model = LDA(n_topics=13, random_state=0)\n",
    "#     model.fit(m.astype(int))\n",
    "\n",
    "    F, p_val = f_classif(m, y_train)\n",
    "    f = open(\"f-scores.txt\", \"w\")\n",
    "    f.write(\"Length of POS: {0}, Length of Vocab: {1}, Length of Candidate: {2}, Length of location: {3}\".format(\n",
    "            len(pos_set), len(vocabulary_set), len(candidate_set), len(location_set)))\n",
    "    for i in range(len(F)):\n",
    "        f.write(\"Score: {0}, Column: {1}\\n\".format(F[i], i))\n",
    "    f.close()\n",
    "    break\n",
    "    \n",
    "#     clf = RandomForestClassifier(random_state=0)\n",
    "#     clf.fit(m, y_train)\n",
    "#     y_pred = clf.predict(m_test)\n",
    "#     error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "#     print error\n",
    "#     averageError += (1./k) * error\n",
    "# print \"Average error: %4.2f%s\" % (100 * averageError,'%')\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
