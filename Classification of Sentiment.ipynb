{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another workbook since the other one was slowing down.\n",
    "# load some prereq libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "import collections\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import word2vec\n",
    "from lda.lda import LDA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from pybrain.structure import FeedForwardNetwork\n",
    "from pybrain.structure import RecurrentNetwork\n",
    "from sklearn.svm import SVC\n",
    "from pybrain.structure import LinearLayer, SigmoidLayer\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from pybrain.structure import FullConnection\n",
    "from pybrain.datasets import ClassificationDataSet\n",
    "from pybrain.utilities           import percentError\n",
    "from pybrain.tools.shortcuts     import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules   import SoftmaxLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################### Library Functions ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(e):\n",
    "    raw = e.decode('utf-8').lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text_no_stem(e):\n",
    "    raw = e.decode('utf-8').lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_num(ys):\n",
    "    y_true = []\n",
    "    for l in ys:\n",
    "        if l == 'Positive':\n",
    "            y_true.append(0)\n",
    "        if l == 'Neutral':\n",
    "            y_true.append(1)\n",
    "        if l == 'Negative':\n",
    "            y_true.append(2)\n",
    "    return y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_vector(tokens, candidate, location, subject):\n",
    "    # returns the feature vector that represents this\n",
    "    v = np.zeros(len(vocabulary_set) + 3)\n",
    "    for t in tokens:\n",
    "        v[vocabulary_set.index(t)] = 1\n",
    "    # get candidate id\n",
    "    v[len(vocabulary_set)] = candidate_set.index(candidate)\n",
    "    v[len(vocabulary_set) + 1] = location_set.index(location)\n",
    "    v[len(vocabulary_set) + 2] = subject_set.index(subject)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_vector_expanded(tokens, candidate, location, subject):\n",
    "    # returns the feature vector that represents this\n",
    "    v = np.zeros(len(vocabulary_set) + len(candidate_set) + len(subject_set) + len(location_set))\n",
    "    for t in tokens:\n",
    "        v[vocabulary_set.index(t)] = 1\n",
    "\n",
    "    # get candidate id\n",
    "    v[len(vocabulary_set) + candidate_set.index(candidate)] = 1\n",
    "    v[len(vocabulary_set) + len(candidate_set) + location_set.index(location)] = 1\n",
    "    v[len(vocabulary_set) + len(candidate_set) + len(location_set) + subject_set.index(subject)] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################## End Library Functions #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('output/Sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dftrain['text']\n",
    "texts_all = []\n",
    "for e in df:\n",
    "    raw = e.decode('utf-8').lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts_all.append(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = [item for sublist in texts_all for item in sublist]\n",
    "# print vocabulary\n",
    "vocabulary_set = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidate_list = []\n",
    "for c in dftrain['candidate']:\n",
    "    candidate_list.append(c)\n",
    "candidate_set = list(set(candidate_list))\n",
    "\n",
    "subject_list = []\n",
    "for s in dftrain['subject_matter']:\n",
    "    subject_list.append(s)\n",
    "subject_set = list(set(subject_list))\n",
    "\n",
    "location_list = []\n",
    "for l in dftrain['tweet_location']:\n",
    "    location_list.append(l)\n",
    "location_set = list(set(location_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a partial set\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(dftrain['text'], dftrain['sentiment'],\n",
    "                                                                    test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build an LDA model using this X_train set\n",
    "m = np.array([np.zeros(len(vocabulary_set)) for i in range(len(X_train))])\n",
    "for i in range(len(X_train)):\n",
    "    tokens = process_text(X_train.iloc[i])\n",
    "    for t in tokens:\n",
    "        m[i][vocabulary_set.index(t)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x10b700b00>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have a matrix, lets use LDA on it.\n",
    "model = LDA(n_topics=10, random_state=0)\n",
    "model.fit(m.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8322, 10)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doc_topic_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have the LDA vectors, which represent the probability of each topic. Lets feed this in as a matrix to a classifier\n",
    "# see how the classifier performs given the dimension reduction that has happened.\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(model.doc_topic_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(dftrain['sentiment'],n_folds=10)\n",
    "averageError = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434125269978\n",
      "0.449244060475\n",
      "0.438040345821\n",
      "0.466474405191\n",
      "0.459985580389\n",
      "0.483056957462\n",
      "0.395382395382\n",
      "0.47113997114\n",
      "0.406204906205\n",
      "0.40404040404\n",
      "Average error: 44.08%\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    m = np.array([np.zeros(len(vocabulary_set)) for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text(X_train.iloc[i])\n",
    "        for t in tokens:\n",
    "            m[i][vocabulary_set.index(t)] = 1\n",
    "    \n",
    "    model = LDA(n_topics=13, random_state=0)\n",
    "    model.fit(m.astype(int))\n",
    "    \n",
    "    m_test = np.array([np.zeros(len(vocabulary_set)) for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text(X_test.iloc[i])\n",
    "        for t in tokens:\n",
    "            m_test[i][vocabulary_set.index(t)] = 1\n",
    "            \n",
    "    clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(model.doc_topic_, y_train)\n",
    "    y_pred = clf.predict(clf_X_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13871\n"
     ]
    }
   ],
   "source": [
    "print len(dftrain['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13871\n"
     ]
    }
   ],
   "source": [
    "print len(dftrain['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n",
      "None of the above\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "for train_index, test_index in skf:\n",
    "    print dftrain['subject_matter'][train_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.429085673146\n",
      "0.462922966163\n",
      "0.448847262248\n",
      "0.452775775054\n",
      "0.433309300649\n",
      "0.381398702235\n",
      "0.383838383838\n",
      "0.4329004329\n",
      "0.430014430014\n",
      "0.450937950938\n",
      "Average error: 43.06%\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    z_array = np.zeros(len(vocabulary_set) + len(candidate_set) + len(location_set) + len(subject_set))\n",
    "    m = np.array([z_array for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        m[i] = create_feature_vector_expanded(tokens, candidate, tweet_location, sm)\n",
    "    \n",
    "    model = LDA(n_topics=13, random_state=0)\n",
    "    model.fit(m.astype(int))\n",
    "    \n",
    "    z_array = np.zeros(len(vocabulary_set) + len(candidate_set) + len(location_set) + len(subject_set))\n",
    "    m_test = np.array([z_array for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        m_test[i] = create_feature_vector_expanded(tokens, candidate, tweet_location, sm)\n",
    "            \n",
    "    clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(model.doc_topic_, y_train)\n",
    "    y_pred = clf.predict(clf_X_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.     0.     0. ...,     8.  1962.    12.]\n",
      " [    0.     0.     0. ...,    11.  2078.    12.]\n",
      " [    0.     0.     0. ...,     8.  3307.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     8.  1670.    12.]\n",
      " [    0.     0.     0. ...,     5.  2691.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    11.]]\n",
      "0.347732181425\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    1.96200000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   1.10000000e+01\n",
      "    2.07800000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    3.30700000e+03   1.20000000e+01]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   5.00000000e+00\n",
      "    1.38900000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   5.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   1.00000000e+01\n",
      "    1.21700000e+03   3.00000000e+00]]\n",
      "0.347732181425\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   3.00000000e+00\n",
      "    1.61900000e+03   2.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   3.00000000e+00\n",
      "    2.98300000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    8.13000000e+02   9.00000000e+00]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   5.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    2.07800000e+03   1.00000000e+00]]\n",
      "0.353025936599\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    4.09100000e+03   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   6.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    3.52800000e+03   3.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   1.00000000e+00\n",
      "    8.40000000e+02   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   6.00000000e+00\n",
      "    4.20100000e+03   2.00000000e+00]]\n",
      "0.366258111031\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     8.  1355.    12.]\n",
      " [    0.     0.     0. ...,     5.  1619.    12.]\n",
      " [    0.     0.     0. ...,     8.  4243.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     8.     0.     5.]\n",
      " [    0.     0.     0. ...,     8.  1222.     5.]\n",
      " [    0.     0.     0. ...,     7.  3777.    12.]]\n",
      "0.360490266763\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     6.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.  1962.    12.]\n",
      " [    0.     0.     0. ...,    11.     0.     9.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     8.  3748.     5.]\n",
      " [    0.     0.     0. ...,     8.  3208.     5.]\n",
      " [    0.     0.     0. ...,     8.     0.     5.]]\n",
      "0.421773612112\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    1.61900000e+03   1.00000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    0.00000000e+00   1.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    1.61900000e+03   1.20000000e+01]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    0.00000000e+00   5.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   8.00000000e+00\n",
      "    3.68600000e+03   5.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   3.00000000e+00\n",
      "    0.00000000e+00   5.00000000e+00]]\n",
      "0.354978354978\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     8.   139.    12.]\n",
      " [    0.     0.     0. ...,     8.  1947.    12.]\n",
      " [    0.     0.     0. ...,     8.  3408.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     8.    97.     5.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.     5.]]\n",
      "0.381673881674\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "[[    0.     0.     0. ...,     8.  1909.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.     3.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     5.  2557.    12.]\n",
      " [    0.     0.     0. ...,     3.   360.    12.]\n",
      " [    0.     0.     0. ...,     8.  2972.    12.]]\n",
      "0.324675324675\n",
      "[[    0.     0.     0. ...,     8.     0.    12.]\n",
      " [    0.     0.     0. ...,     4.     0.    12.]\n",
      " [    0.     0.     0. ...,     8.     0.    12.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,     5.  2557.    12.]\n",
      " [    0.     0.     0. ...,     3.   360.    12.]\n",
      " [    0.     0.     0. ...,     8.  2972.    12.]]\n",
      "[[    0.     0.     0. ...,     6.     0.    11.]\n",
      " [    0.     0.     0. ...,     8.   166.     6.]\n",
      " [    0.     0.     0. ...,     3.     0.     5.]\n",
      " ..., \n",
      " [    0.     0.     0. ...,    11.     0.    12.]\n",
      " [    0.     0.     0. ...,     5.     0.    11.]\n",
      " [    0.     0.     0. ...,    11.  2201.    12.]]\n",
      "0.351370851371\n",
      "Average error: 36.10%\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    m = np.array([np.zeros(len(vocabulary_set) + 3) for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        m[i] = create_feature_vector(tokens, candidate, tweet_location, sm)\n",
    "    print m\n",
    "#     model = LDA(n_topics=13, random_state=0)\n",
    "#     model.fit(m.astype(int))\n",
    "    \n",
    "    m_test = np.array([np.zeros(len(vocabulary_set) + 3) for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        m_test[i] = create_feature_vector(tokens, candidate, tweet_location, sm)\n",
    "    print m_test\n",
    "#     clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    clf.fit(m, y_train)\n",
    "    y_pred = clf.predict(m_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    m = np.array([np.zeros(len(vocabulary_set) + 3) for i in range(len(X_train))])\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        m[i] = create_feature_vector(tokens, candidate, tweet_location, sm)\n",
    "    print m\n",
    "#     model = LDA(n_topics=13, random_state=0)\n",
    "#     model.fit(m.astype(int))\n",
    "    \n",
    "    m_test = np.array([np.zeros(len(vocabulary_set) + 3) for i in range(len(X_test))])\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        m_test[i] = create_feature_vector(tokens, candidate, tweet_location, sm)\n",
    "    print m_test\n",
    "#     clf_X_test = model.transform(m_test.astype(int))\n",
    "    \n",
    "    clf = SVC(kernel=chi2_kernel)\n",
    "    clf.fit(m, y_train)\n",
    "    y_pred = clf.predict(m_test)\n",
    "    error = zero_one_loss(label_to_num(y_test), label_to_num(y_pred))\n",
    "    print error\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## building a neural network system to test with here as well.\n",
    "def createNetwork(inputDim, hiddenDim, outputDim):\n",
    "    n = RecurrentNetwork()\n",
    "    n.addInputModule(LinearLayer(inputDim, name='in'))\n",
    "    n.addModule(SigmoidLayer(hiddenDim, name='hidden'))\n",
    "    n.addModule(SigmoidLayer(hiddenDim, name='hidden1'))\n",
    "    n.addModule(SigmoidLayer(hiddenDim, name='hidden2'))\n",
    "    n.addOutputModule(SoftmaxLayer(outputDim, name='out'))\n",
    "    n.addConnection(FullConnection(n['in'], n['hidden'], name='c1'))\n",
    "    n.addConnection(FullConnection(n['hidden'], n['hidden1'], name='c2'))\n",
    "    n.addConnection(FullConnection(n['hidden1'], n['hidden2'], name='c3'))\n",
    "    n.addConnection(FullConnection(n['hidden2'], n['out'], name='c4'))\n",
    "    n.addRecurrentConnection(FullConnection(n['hidden2'], n['hidden'], name='c5'))\n",
    "    n.sortModules()\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: dim(16382, 19668)\n",
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "\n",
      "target: dim(16382, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [2]\n",
      " [0]]\n",
      "\n",
      "class: dim(0, 1)\n",
      "[]\n",
      "\n",
      "\n",
      "input: dim(2046, 19668)\n",
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "\n",
      "target: dim(2046, 1)\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "\n",
      "class: dim(0, 1)\n",
      "[]\n",
      "\n",
      "\n",
      "RecurrentNetwork-20\n",
      "   Modules:\n",
      "    [<LinearLayer 'in'>, <SigmoidLayer 'hidden'>, <SigmoidLayer 'hidden1'>, <SigmoidLayer 'hidden2'>, <SoftmaxLayer 'out'>]\n",
      "   Connections:\n",
      "    [<FullConnection 'c1': 'in' -> 'hidden'>, <FullConnection 'c2': 'hidden' -> 'hidden1'>, <FullConnection 'c3': 'hidden1' -> 'hidden2'>, <FullConnection 'c4': 'hidden2' -> 'out'>]\n",
      "   Recurrent Connections:\n",
      "    [<FullConnection 'c5': 'hidden2' -> 'hidden'>]\n",
      "Training Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(label_to_num(dftrain['sentiment']),n_folds=k)\n",
    "averageError = 0.0\n",
    "for train_index, test_index in skf:\n",
    "    dftrain_cols = zip(dftrain['text'], dftrain['candidate'], dftrain['tweet_location'], dftrain['subject_matter'])\n",
    "    X_train, X_test = dftrain['text'][train_index], dftrain['text'][test_index]\n",
    "    y_train, y_test = dftrain['sentiment'][train_index], dftrain['sentiment'][test_index]\n",
    "    \n",
    "    # create the LDA transformation first\n",
    "    input_dim = len(vocabulary_set) + len(candidate_set) + len(location_set) + len(subject_set)\n",
    "    trndata = ClassificationDataSet(input_dim, nb_classes=3)\n",
    "    for i in range(len(X_train)):\n",
    "        tokens = process_text(X_train.iloc[i])\n",
    "        candidate = dftrain['candidate'][train_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][train_index[i]]\n",
    "        sm = dftrain['subject_matter'][train_index[i]]\n",
    "        l = label_to_num([y_train.iloc[i]])[0]\n",
    "        trndata.addSample(create_feature_vector_expanded(tokens, candidate, tweet_location, sm), [l])\n",
    "    \n",
    "    print trndata    \n",
    "    \n",
    "    testdata = ClassificationDataSet(input_dim, nb_classes=3)\n",
    "    for i in range(len(X_test)):\n",
    "        tokens = process_text(X_test.iloc[i])\n",
    "        candidate = dftrain['candidate'][test_index[i]]\n",
    "        tweet_location = dftrain['tweet_location'][test_index[i]]\n",
    "        sm = dftrain['subject_matter'][test_index[i]]\n",
    "        l = label_to_num([y_test.iloc[i]])[0]\n",
    "        testdata.addSample(create_feature_vector_expanded(tokens, candidate, tweet_location, sm), [l])\n",
    "    \n",
    "    print testdata\n",
    "    # convert to one of many\n",
    "    trndata._convertToOneOfMany()\n",
    "    testdata._convertToOneOfMany()\n",
    "    # train the network\n",
    "    n = createNetwork(trndata.indim, 100, trndata.outdim)\n",
    "    print n\n",
    "    trainer = BackpropTrainer(n, dataset=trndata, momentum=0.1, verbose=True, weightdecay=0.01)\n",
    "    \n",
    "    # train for a certain amount of epochs\n",
    "    for i in range(20):\n",
    "        print \"Training Epoch: {0}\".format(i)\n",
    "        trainer.trainEpochs(5)\n",
    "        trnresult = percentError( trainer.testOnClassData(),\n",
    "                              trndata['class'] )\n",
    "        tstresult = percentError( trainer.testOnClassData(\n",
    "               dataset=testdata ), testdata['class'] )\n",
    "\n",
    "        print \"epoch: %4d\" % trainer.totalepochs, \\\n",
    "              \"  train error: %5.2f%%\" % trnresult, \\\n",
    "              \"  test error: %5.2f%%\" % tstresult\n",
    "    error = percentError(trainer.testOnClassData(dataset=testdata), testdata['class'])\n",
    "    averageError += (1./k) * error\n",
    "print \"Average error: %4.2f%s\" % (100 * averageError,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
